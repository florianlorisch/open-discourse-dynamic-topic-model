{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import unidecode\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import Phrases\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('de_core_news_lg')  # requires python3 -m spacy download de_core_news_lg in terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "data_path = os.path.join(os.path.abspath(os.curdir), 'corpus','prepared','corpus.csv')\n",
    "result_path = os.path.join(os.path.abspath(os.curdir),'corpus','preprocessed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loading prepared dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path)\n",
    "# sort dataframe by id, same structures is followed to create time slice list\n",
    "df.sort_values(by='id', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Preprocessing the plenary speeches for LDA and DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def save_terms_to_disk(df):\n",
    "    '''\n",
    "    :param df: dataframe containing speeches and an 'electoralTerms' column\n",
    "    :return: Save electoral terms that are covered within the dataset to disk. This is needed as input for DTM\n",
    "    '''\n",
    "    path = os.path.join(result_path, \"electoralTerms\")\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    file_name = os.path.join(path, \"electoralTerms_list.pkl\")\n",
    "    electoralTerms_list = df[\"electoralTerm\"].tolist()\n",
    "    with open(file_name, 'wb') as handle:\n",
    "        pickle.dump(electoralTerms_list, handle)\n",
    "\n",
    "    file_name = os.path.join(path, \"electoralTerms_count.pkl\")\n",
    "    electoralTerms_count = df['electoralTerm'].value_counts()\n",
    "    with open(file_name, 'wb') as handle:\n",
    "        pickle.dump(electoralTerms_count, handle)\n",
    "\n",
    "save_terms_to_disk(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Initial data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def data_cleaning(df):\n",
    "    '''\n",
    "\n",
    "    :param df: dataframe prepared during the first preprocessing step\n",
    "    :return: dataframe with a column that contains lowercased plenary speeches without puncutation\n",
    "    '''\n",
    "    \"\"\"\n",
    "    This function will apply a number of lambda functions over a pandas series such as df['text'].\n",
    "    Data_Cleaning will convert text to lowercase and remove punctuation\n",
    "    \"\"\"\n",
    "    speechContent_column = df['speechContent']\n",
    "\n",
    "    # convert to lowercase\n",
    "    speechContent_column = speechContent_column.apply(lambda x: ' '.join([w.lower() for w in x.split()]))\n",
    "\n",
    "    # remove punctuation\n",
    "    speechContent_column = speechContent_column.apply(lambda x: ' '.join(re.sub(\"[\\W]\", \" \", x).split()))\n",
    "\n",
    "\n",
    "    return speechContent_column\n",
    "\n",
    "df['speechContent_cleaned'] = data_cleaning(df=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Tokenization of plenary speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_speeches(df):\n",
    "    '''\n",
    "    :param df: dataframe with cleaned speech in a column 'speechContent_cleaned'\n",
    "    :return: list of list with tokenized speeches\n",
    "    '''\n",
    "\n",
    "    speeches = df['speechContent_cleaned'].values.tolist()\n",
    "    for speech in tqdm(speeches, unit=\"speeches\", desc=\"Extract tokens\"):\n",
    "        yield (gensim.utils.simple_preprocess(str(speech),\n",
    "                                              deacc=False,\n",
    "                                              min_len=2))\n",
    "\n",
    "tokenized_speeches = list(tokenize_speeches(df))\n",
    "print(\"Total number of speeches:\", len(tokenized_speeches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Removing general and custom stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_general_stopwords(tokenized_speeches):\n",
    "    '''\n",
    "    :param tokenized_speeches: list of list with tokenized speeches\n",
    "    :return: uses spaCy default stopwords from 'de_core_news_lg' and removes them from speeches\n",
    "    '''\n",
    "    stopwords = nlp.Defaults.stop_words\n",
    "    tokenized_speeches = [[token for token in speech if token not in stopwords] for speech in tqdm(tokenized_speeches)]\n",
    "    return tokenized_speeches, stopwords\n",
    "\n",
    "tokenized_speeches, stopwords = remove_general_stopwords(tokenized_speeches)\n",
    "print(\"Number of stopwords: \", len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_word_frequency(tokenized_speeches):\n",
    "    '''\n",
    "    :param tokenized_speeches: list of list with tokenized speeches after general stopwords are removed\n",
    "    :return: dataframe with word frequencies\n",
    "    '''\n",
    "    flat_list = [token for speeches in tokenized_speeches for token in speeches]\n",
    "    flat_count = dict(Counter(flat_list))\n",
    "    word_frequency_after_stopwords = pd.DataFrame.from_dict(flat_count, orient='index', columns=['count'])\n",
    "    word_frequency_after_stopwords.sort_values(by='count', ascending=False, inplace=True)\n",
    "    word_frequency_after_stopwords = word_frequency_after_stopwords.reset_index()\n",
    "    word_frequency_after_stopwords.columns = ['word', 'count']\n",
    "    return word_frequency_after_stopwords\n",
    "\n",
    "word_frequency = get_word_frequency(tokenized_speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_initial_custom_stopword_set(word_frequency, number_of_stopwords):\n",
    "    '''\n",
    "    :param word_frequency: dataframe containing the word frequencies, with a column 'word' that contains one word per row\n",
    "    :param number_of_stopwords: number of most frequent words that should be taken as initial stopwords\n",
    "    :return: an initial set of stopwords containing the n most frequent words in the tokenized speeches\n",
    "    '''\n",
    "    custom_stopword_set = set(word_frequency['word'].head(n=number_of_stopwords))\n",
    "    return custom_stopword_set\n",
    "\n",
    "initial_custom_stopword_set = create_initial_custom_stopword_set(word_frequency, number_of_stopwords = 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_final_custom_stopword_set(initial_custom_stopword_set, words_to_keep):\n",
    "    '''\n",
    "    :param initial_custom_stopword_set: initial_custom_stopword_set created earlier\n",
    "    :param words_to_keep: relevant words that are added to a txt file after analysing the initial_custom_stopword_set\n",
    "    :return: final_custom_stopword_set with words that should be deleted before further processing\n",
    "    '''\n",
    "    custom_stopword_set = initial_custom_stopword_set - words_to_keep\n",
    "    return custom_stopword_set\n",
    "\n",
    "words_to_keep = set(line.strip() for line in open(os.path.join(result_path, \"stopwords\", \"words_to_keep.txt\")))\n",
    "custom_stopword_set = create_final_custom_stopword_set(initial_custom_stopword_set, words_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_custom_stopwords(tokenized_speeches, custom_stopword_set):\n",
    "    '''\n",
    "    :param tokenized_speeches: list of list with tokenized speeches after general stopwords are removed\n",
    "    :param custom_stopword_set: spaCy default stopwords appended with custom stop words\n",
    "    :return: list of list with tokenized speeches without stopwords\n",
    "    '''\n",
    "    nlp.Defaults.stop_words |= custom_stopword_set\n",
    "    stopwords_custom = nlp.Defaults.stop_words\n",
    "    print(\"Number of custom stopwords: \", len(stopwords_custom))\n",
    "\n",
    "    speeches_tokens = [[token for token in speech if token not in stopwords_custom] for speech in\n",
    "                                         tqdm(tokenized_speeches)]\n",
    "    return tokenized_speeches, stopwords_custom\n",
    "\n",
    "tokenized_speeches, stopwords_custom = remove_custom_stopwords(tokenized_speeches, custom_stopword_set)\n",
    "word_frequency_after_custom_stopwords = get_word_frequency(tokenized_speeches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Removing umlauts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_umlaute(tokenized_speeches):\n",
    "    '''\n",
    "    :param tokenized_speeches: list of list with tokenized speeches without stopwords\n",
    "    :return: list of list with tokenized speeches without stopwords, umlauts, and accents\n",
    "    '''\n",
    "    speeches_tokens = [[unidecode.unidecode(token) for token in speech] for speech in tqdm(tokenized_speeches)]\n",
    "    return speeches_tokens\n",
    "\n",
    "speeches_tokens = remove_umlaute(tokenized_speeches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing plenary speeches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize(tokenized_speeches, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    '''\n",
    "    :param tokenized_speeches: list of list with tokenized speeches after general and custom stopwords are removed\n",
    "    :param allowed_postags: Which types of words should be kept, default 'NOUN', 'ADJ', 'VERB', 'ADV'\n",
    "    :return: lemmatized speeches\n",
    "    '''\n",
    "    lemmatized_speeches = [[item.lemma_ for item in nlp(' '.join(speech)) if item.pos_ in allowed_postags] for speech\n",
    "                           in tqdm(tokenized_speeches,\n",
    "                                   unit=\"speech\",\n",
    "                                   desc=\"Creating lemmas\")]\n",
    "    return lemmatized_speeches\n",
    "\n",
    "\n",
    "lemmatized_speeches = lemmatize(tokenized_speeches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding bigrams to the lemmatized plenary speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_bigrams(lemmatized_speeches, name):\n",
    "    '''\n",
    "\n",
    "    :param lemmatized_speeches: lemmatized speeches as list of list\n",
    "    :param name: filename for the bigram model\n",
    "    :return:\n",
    "    '''\n",
    "    bigram = gensim.models.Phrases(lemmatized_speeches, min_count=400,threshold= 30)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    for idx in tqdm(range(len(lemmatized_speeches))):\n",
    "        for token in bigram_mod[lemmatized_speeches[idx]]:\n",
    "            if '_' in token:\n",
    "                lemmatized_speeches[idx].append(token)\n",
    "    bigram_mod.save(os.path.join(result_path,'ngram_models', name))\n",
    "    return lemmatized_speeches\n",
    "\n",
    "name = 'bigram_model.pkl'\n",
    "speeches_bigrams = get_bigrams(lemmatized_speeches=lemmatized_speeches, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Create a dictionary and corpus from the lemmatized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#create a dictionary and corpus from the full lemmatized speeches\n",
    "dictionary = Dictionary(speeches_bigrams)\n",
    "corpus_full = [dictionary.doc2bow(speech) for speech in tqdm(speeches_bigrams)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filter_extremes(dictionary, no_below, no_above):\n",
    "    '''\n",
    "    :param dictionary: full dictionary\n",
    "    :param no_below: (int) – Keep tokens which are contained in at least no_below documents.\n",
    "    :param no_above: (float) – Keep tokens which are contained in no more than no_above documents (fraction of total corpus size, not an absolute number)\n",
    "    :return: pruned dicitionary\n",
    "    '''\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "    dictionary.compactify()\n",
    "    return dictionary\n",
    "\n",
    "filter_extremes(dictionary, no_below=10, no_above=0.5)\n",
    "\n",
    "\n",
    "path = os.path.join(result_path, \"corpus\")\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "file_name = os.path.join(path, \"corpus_full.pkl\")\n",
    "with open(file_name, 'wb') as handle:\n",
    "    pickle.dump(corpus_full, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prune_lemmas_with_pruned_dict(dict, speech_tokens):\n",
    "    '''\n",
    "    :param dict: pruned dictionary\n",
    "    :return: pruned lemmatized tokens, which can be used as input for the coherencemodel later on\n",
    "    '''\n",
    "    dict_tokens = dict.token2id\n",
    "    dict_keys = list(dict_tokens.keys())\n",
    "    lemmatized_filtered = [[token for token in speeches if token in dict_keys] for speeches in\n",
    "                                tqdm(speech_tokens)]\n",
    "\n",
    "    return lemmatized_filtered\n",
    "\n",
    "lemmatized_filtered = prune_lemmas_with_pruned_dict(dictionary,speeches_bigrams)\n",
    "\n",
    "path = os.path.join(result_path, \"lemmas\")\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "file_name = os.path.join(path, \"lemmatized_preprocessed.pkl\")\n",
    "with open(file_name, 'wb') as handle:\n",
    "    pickle.dump(lemmatized_filtered, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##% create a filtered corpus\n",
    "corpus_preprocessed = [dictionary.doc2bow(speech) for speech in tqdm(speeches_bigrams)]    \n",
    "\n",
    "path = os.path.join(result_path, \"corpus\")\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "file_name = os.path.join(path, \"corpus_preprocessed.pkl\")\n",
    "with open(file_name, 'wb') as handle:\n",
    "    pickle.dump(corpus_preprocessed, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Update the preprocessed dictionary with additional stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#load initial dictionary from disk\n",
    "path = os.path.join(result_path, \"dictionary\")\n",
    "loaded_dict = Dictionary.load(os.path.join(path, \"dictionary_preprocessed.dict\"))\n",
    "\n",
    "#load additional stopwords that were identified during initial attempts to LDA modeling\n",
    "additional_stopwords = list(line.strip() for line in open(os.path.join(result_path, \"stopwords\",\"additional_stopwords.txt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def delete_additional_stopwords_from_dict (dict, additional_stopwords):\n",
    "    '''\n",
    "    \n",
    "    :param dict: Initially created dictionary (loaded from disk)\n",
    "    :param additional_stopwords: \n",
    "    :return: \n",
    "    '''\n",
    "    stopword_ids = map(dict.token2id.get, additional_stopwords)\n",
    "    dict.filter_tokens(bad_ids=stopword_ids)\n",
    "    dict.compactify()\n",
    "    \n",
    "    for x in additional_stopwords:\n",
    "        if x in dict.token2id:\n",
    "            print(x,'yes')\n",
    "        else:\n",
    "            print(x,'no')\n",
    "    return dict\n",
    "\n",
    "dictionary_updated = delete_additional_stopwords_from_dict (dict=loaded_dict, additional_stopwords=additional_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Update corpus after removing additional stopwords from dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "texts = pd.read_pickle(os.path.join(result_path, 'preprocessed_lemmas', 'lemmatized_preprocessed.pkl'))\n",
    "\n",
    "lemmatized_speeches_pruned = prune_lemmas_with_pruned_dict(dictionary_updated,texts)\n",
    "\n",
    "corpus_updated = [dictionary_updated.doc2bow(speech) for speech in tqdm(lemmatized_speeches_pruned)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Save updated lemmas, corpus, and dictionary to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = os.path.join(result_path, 'lemmas')\n",
    "os.makedirs(path, exist_ok=True)\n",
    "file_name = os.path.join(path, \"lemmatized_preprocessed.pkl\")\n",
    "with open(file_name, 'wb') as handle:\n",
    "    pickle.dump(lemmatized_speeches_pruned, handle)\n",
    "\n",
    "path = os.path.join(result_path, \"corpus\")\n",
    "os.makedirs(path, exist_ok=True)\n",
    "file_name = os.path.join(path, \"corpus_preprocessed.pkl\")\n",
    "with open(file_name, 'wb') as handle:\n",
    "    pickle.dump(corpus_updated, handle)\n",
    "\n",
    "path = os.path.join(result_path, \"dictionary\")\n",
    "dictionary.save(os.path.join(path, \"dictionary_preprocessed.dict\"))\n",
    "dictionary.save_as_text(os.path.join(path, \"dictionary_preprocessed.txt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}