{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "#BERTopic related imports\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "import hdbscan\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "\n",
    "#sklean imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setting paths"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.chdir(\"../..\")\n",
    "data_path = os.path.join(os.path.abspath(os.curdir), 'corpus','preprocessed')\n",
    "result_path = os.path.join(os.path.abspath(os.curdir),'models','BERTopic')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading pre-processed files from disk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_name = os.path.join(data_path,'electoralTerms', 'BERTopic_time_steps.pkl')\n",
    "with open(file_name, 'rb') as pickle_file:\n",
    "    speeches = pickle.load(pickle_file)\n",
    "\n",
    "file_name = os.path.join(data_path,'corpus', 'BERTopic_corpus_preprocessed.pkl')\n",
    "with open(file_name, 'rb') as pickle_file:\n",
    "    time_steps = pickle.load(pickle_file)\n",
    "\n",
    "file_name = os.path.join(data_path,'stopwords', 'stopwords_custom.pkl')\n",
    "with open(file_name, 'rb') as pickle_file:\n",
    "    stopwords = pickle.load(pickle_file)\n",
    "\n",
    "file_name = os.path.join(data_path,'stopwords', 'additional_stopwords.txt')\n",
    "additional_stopwords = list(line.strip() for line in open(file_name))\n",
    "stopwords.update(additional_stopwords)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Instantiating the components of BERTopic"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Instantiate vectorizer model and pass it the complete set of stopwords used for LDA and DTM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vectorizer_model = CountVectorizer(stop_words=stopwords,min_df=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Instantiate sentenceTransformer wit pre-tranined multilingual model and create embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentence_model = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "embeddings = sentence_model.encode(speeches, show_progress_bar=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Instantiate UMAP Model with n_neighbors = 100 (default = 15) and random_state=41"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "umap_model = UMAP(n_neighbors=100, n_components=5,\n",
    "                  min_dist=0.0, metric='cosine',\n",
    "                  random_state=41)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Instantiate HDBSCAN model with min_cluster_size = 300 (default = 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hdbscan_model = HDBSCAN(min_cluster_size=300, metric='euclidean',\n",
    "                        cluster_selection_method='eom', prediction_data=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training BERTopic"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "topic_model = BERTopic(embedding_model=sentence_model,\n",
    "                       umap_model=umap_model,\n",
    "                       vectorizer_model=vectorizer_model,\n",
    "                       hdbscan_model=hdbscan_model,\n",
    "                       min_topic_size=300,\n",
    "                       top_n_words=25,\n",
    "                       nr_topics=31,\n",
    "                       calculate_probabilities=True,\n",
    "                       verbose=True)\n",
    "topics, probs = topic_model.fit_transform(documents=speeches,embeddings=embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modeling topics over time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "topics_over_time = topic_model.topics_over_time(docs=speeches,\n",
    "                                                topics=topics,\n",
    "                                                timestamps=time_steps,\n",
    "                                                global_tuning=True,\n",
    "                                                evolution_tuning=True,\n",
    "                                                nr_bins=19)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Saving trained BERTopic and topics over time to disk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_name = os.path.join(result_path,'model_results', 'BERTopic')\n",
    "topic_model.save(file_name)\n",
    "\n",
    "file_name = os.path.join(result_path,'model_results', 'topics_over_time.pkl')\n",
    "with open(file_name, 'wb') as handle:\n",
    "     pickle.dump(topics_over_time, handle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}